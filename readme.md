# ğŸš€ DocuMind â€“ Async RAG Pipeline

DocuMind is a fully asynchronous **Retrieval-Augmented Generation (RAG)** system built from scratch to explore scalable GenAI system design.

Instead of directly calling an LLM synchronously, this system uses a **job-based async architecture** powered by queues and background workers.

---

# ğŸ—ï¸ Architecture Overview

## ğŸ”„ Request Flow

1. User sends request to `/chat`
2. API enqueues job in **Valkey (Redis-compatible)**
3. Worker processes:
   - Convert query â†’ embeddings
   - Perform similarity search in **Qdrant**
   - Prepare contextual prompt
   - Call OpenAI LLM
4. Store result in Valkey
5. Client polls `/result/{job_id}`

---

## âœ… Why This Architecture?

- Scalable
- Prevents request timeouts
- Clean separation of API & heavy LLM tasks
- Production-ready async workflow
- Queue-based background processing

---

# ğŸ› ï¸ Tech Stack

- âš¡ **FastAPI** â€“ Backend API  
- ğŸ”´ **Valkey (Redis-compatible)** â€“ Queue + Result Store  
- ğŸ” **RQ (Redis Queue)** â€“ Background workers  
- ğŸ§  **Qdrant** â€“ Vector database  
- ğŸ¤– **OpenAI API** â€“ Embeddings + LLM  

---


# ğŸ³ Running with Docker Compose

Make sure you are in the root directory (where `docker-compose.yml` exists).

Start services:

```bash
docker compose up -d
```
Stop services:

```bash
docker compose down
```

## ğŸ” Environment Variables

Create a .env file in the root directory:

OPENAI_API_KEY=your_openai_key

## â–¶ï¸ Running the Backend

```bash
python -m main.py
```
ğŸ‘· Running the Worker

In a separate terminal:
```bash
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES 
rq worker --with-scheduler
```
ğŸ“¡ API Endpoints
POST /chat

Submit user query.

Returns:

{
  "job_id": "xxxx"
}
GET /result/{job_id}

Fetch processed result.

## ğŸ”® Future Improvements

Streaming responses

Caching layer

Observability (metrics + tracing)

Rate limiting

Frontend UI

Authentication layer

## ğŸ’¡ Key Learning

GenAI systems are not just about prompting.

They require:

Proper async architecture

Background processing

Queue management

Vector search optimization

Clean separation of concerns
